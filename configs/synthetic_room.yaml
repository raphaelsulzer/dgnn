features:
  scaling: [s]
  normalization_range: [0,1]
  node_features: [shape, vertex, facet, count, min, max, sum, first, second]
  edge_features: [shape, vertex, facet, count, min, max, sum]
  node_normalization_feature: null
  edge_normalization_feature: null
graph:
  num_hops: 4 # outer hops (unsupervised) == num_layers
  additional_num_hops: 0 # inner hops (supervised), this is there for having a supervised connected component, if set to 0, only the last layer of num_hops is supervised in the form of unconnected nodes
  clique_sizes: [-1] # -1 meaning sample all existing nodes
  self_loops: 0
inference:
  dataset: synthetic_room_dataset
  classes: null
  scan_confs: [990]
  shapes_per_conf_per_class: 10
  files: null
  batch_size: 0
  per_layer: 1
  has_label: 1
  model: 20
  graph_cut: false
  fix_orientation: false
  metrics: [ "iou" ]
  export: [ "mesh" ]
validation:
  dataset: synthetic_room_dataset
  classes: null
  scan_confs: [990]
  shapes_per_conf_per_class: 5
  batch_size: 0
  per_layer: 1
  graph_cut: true
  fix_orientation: true
  metrics: [ "iou" ]
training:
  dataset: synthetic_room_dataset
  classes: null
  scan_confs: [990]
  data_percentage: 0.6
  shapes_per_conf_per_class: 5
  batch_size: 128
  epochs: 100
  loss: kl
  learning_rate: 0.005
  adjust_lr_every: 24
  load_epoch: 0
  print_every: 1000
  val_every: 1000
  export_every: 20000
model:
  type: sage
  encoder: null
  convs: [64, 128, 128, 128]
  edge_convs: 1
  decoder: 2
  concatenate: 0 # whether to concatenate (or add) target to source node embedding
  edge_prediction: 0
  normalization: b
paths:
  data: /home/rsulzer/data/synthetic_room_dataset
  out: /home/rsulzer/data/synthetic_room_out/dgnn/debug
regularization:
  cell_type: vol
  cell_norm: sqrt # [null, sqrt, log]
  edge_type: null # edge regularization
  edge_epoch: null # edge regularization
  edge_weight: 0.4 # edge regularization
graph_cut:
  unary_weight: 10
  unary_type: logits
  binary_weight: 10
  binary_type: beta
